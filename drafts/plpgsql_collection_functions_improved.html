<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
    "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Batch Updating in PL/pgSQL</title>
    <meta name="keywords" content="PostgreSQL, PL/pgSQL, functions, performance, SQL, database, performance" />
    <style type="text/css">
      pre      { background:#ddd; padding: 1em; border: 1px solid black; }
      dfn      { font-style: italic; } /* Some browsers don't do this automatically */
      .linenos { text-align: right; }
    </style>
  </head>
  <body>
    <h1>Batch Updating in PL/pgSQL</h1>

<p>In the <a href="plpgsql_collection_functions.html" title="Practical
PL/pgSQL: Managing Ordered Sets">previous article</a> in this series, we
created four functions to simplify the management of ordered collections as
many-to-many relationships. The two more complex functions,
<code>entry_coll_tag_set()</code> and <code>entry_coll_tag_add()</code>, each
take an iterative approach to managing those relationships. And by
<q>iterative</q> I mean that they used loops to iterate over an array of IDs
in order to do the <q>right thing</q> for each.</p>

<p>The downside to this approach is that the performance of those functions is
directly proportional to the number of IDs in the array
(<code><a href="http://en.wikipedia.org/wiki/Big_O_notation" title="Wikipedia:
Big O Notation">&#x039f;</a>(n)</code>). What would be ideal would be to make
the run time of the functions constant regardless of the number of IDs in the
array (<code>&#x039f;(1)</code>).</p>

<p>Fortunately, there is a way to do just that in PostgreSQL, but first we
need to go back to the Fibonacci examples from the
<a href="plpgsql_functions.html" title="Introduction to PostgreSQL
PL/pgSQL">first article</a> in this series to introduce some new concepts in a
simpler format than the collection functions allow.</p>

<h2>Set Returning Functions</h2>

<p>As I mentioned in that <a href="plpgsql_functions.html" title="Introduction
to PostgreSQL PL/pgSQL">first article</a>, PostgreSQL functions can return a
value of any supported data type. But what I didn't mention is that they can
also return sets of a particular type. A <dfn>set</dfn> is a list of values of
a particular type, but rather than returning them as a <em>list</em> or an
array, as one might expect in a dynamic programming language, PostgreSQL
functions return them as <em>rows</em> of data.</p>

<p>Let's look at an example. Say that you needed to get a list of Fibonacci
numbers up to a particular place in the Fibonacci sequence. Writing such a
function in PL/pgSQL is as simple as modifying the <code>fib_fast()</code>
function to return each Fibonacci number it calculates it. It does so using
the PL/pgSQL <code>RETURN NEXT</code> statement. Let's take a look at
<code>fib_fast()</code> side-by-side with the new set-returning function,
<code>fibs_to()</code>:</p>

<table>
<tr><td><pre class="linenos">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td><td>
<pre>
CREATE OR REPLACE FUNCTION fib_fast(
    fib_for integer
) RETURNS integer AS $$
DECLARE
    ret integer := 0;
    nxt integer := 1;
    tmp integer;
BEGIN
    FOR num IN 1..fib_for LOOP

        tmp := ret;
        ret := nxt;
        nxt := tmp + nxt;
    END LOOP;

    RETURN ret;
END;
$$ LANGUAGE plpgsql;
</pre>
</td><td>
<pre>
CREATE OR REPLACE FUNCTION fibs_to(
    max_num integer
) RETURNS <strong>SETOF</strong> integer AS $$
DECLARE
    ret integer := 0;
    nxt integer := 1;
    tmp integer;
BEGIN
    FOR num IN 1..max_num LOOP
        <strong>RETURN NEXT ret;</strong>
        tmp := ret;
        ret := nxt;
        nxt := tmp + nxt;
    END LOOP;

    RETURN <strong>NEXT</strong> ret;
END;
$$ LANGUAGE plpgsql;
</pre>
</td></tr></table>

<p>There are really only three differences aside from the function names, and
I've emphasized them in <code>fibs_to()</code>. The first difference is on
line three, where <code>fibs_to()</code> is declared to return a <code>SETOF
integer</code> instead of simply an <code>integer</code>. The
<code>SETOF</code> keyword is what tells PostgreSQL that this function returns
a set.</p>

<p>The only other changes to notice are that, rather than simply returning the
value of the <code>ret</code> integer variable, <code>fibs_to()</code> uses
the <code>RETURN NEXT</code> statement to return each Fibonacci number as it
is calculated in the loop. The final <code>RETURN NEXT</code> statement of
course returns the final Fibonacci number in the sequence.</p>

<p>That's all that needs to be changed to create a set returning function, but
as such function, the <code>fibs_to()</code> must also be called in a
different context. While <code>fib_fast()</code> can simply be called in
a <code>SELECT</code> statement:</p>

<pre>
try=% <strong>select fib_fast(8);</strong>
 fib_fast 
----------
       21
(1 row)
</pre>

<p><code>fib_to()</code>, since it essentially behaves like a table, must be
treated as such by using it in a <code>FROM</code> clause:</p>

<pre>
try=% <strong>select * from fibs_to(8);</strong>
 fibs_to 
---------
       0
       1
       1
       2
       3
       5
       8
      13
      21
(9 rows)
</pre>

<p>Be warned, however, that while it <em>looks</em>
like <code>fibs_to()</code> behaves like
a <a href="http://www.sidhe.org/~dan/blog/archives/000156.html" title="Squawks
of the Parrot: Continuations and VMs">continuation</a> and for most practical
purposes may be treated
like <a href="http://en.wikipedia.org/wiki/Continuation" title="Wikipedia:
Continuation">one</a>, what PostgreSQL actually does is buffer all of the
values returned by <code>RETURN NEXT</code> and only return them to the
calling context when they have all been calculated. But all that means is that
if you write a set returning function that returns a <em>lot</em> of values,
you'll want to make sure that your server's memory can handle it.</p>

<p>But that caveat aside, set returning functions can be <em>extremely</em>
useful, as we will shortly see.</p>


<h2>Batch Update Syntax</h2>

<p>PostgreSQL supports the ability to insert multiple rows at once using its
special <code>INSERT INTO…<em>expression</em></code> syntax, which essentially
allows you to select a series of rows, and to turn around and insert them
elsewhere, all in a single statement. For example, say that you wanted to copy
all of the rows in table <var>foo</var> into table <var>bar</var> that weren't
already in <var>bar</var>. Here's how to do it in a single <code>INSERT</code>
statement:</p>

<pre>
INSERT INTO bar (a, b, c)
SELECT d, e, f FROM foo
WHERE  foo.d not in (SELECT a FROM bar);
</pre>

<p>That's it. Since we're using a <code>SELECT</code> statement in the
expression part of the <code>INSERT</code> statement, we're able to insert
multiple rows in a single statement, and even limit the rows through the use
of the <code>WHERE</code> clause. Of course I'm sure that <em>you</em> don't
duplicate data like this unless you're deprecating table <code>foo</code> in
favor of table <code>bar</code>, right? But this syntax does have its uses, as
we shall see.</p>

<p>As for batch updates, you likely already know that you can update multiple
rows at once using a <code>WHERE</code> clause in an <code>UPDATE</code>
statement. But as with <code>INSERT</code>, <code>UDPATE</code> can also
update a series of rows from an expression, generally a <code>SELECT</code>
statement. This is possible because, in PostgreSQL, a <code>FROM</code> clause
can be an expression, and <code>UPDATE</code> supports the <code>FROM</code>
clause. For example, say we wanted to update all of the rows in our
<var>bar</var> table with values from the <var>foo</var> table where
<var>bar.a</var> has the same value as <var>foo.d</var>. Here's how it's
done:</p>

<pre>
UPDATE bar
SET    b = foo.e, c = foo.f
FROM   (SELECT d, e, f FROM foo) AS foo
WHERE  a = foo.d;
</pre>

<p>Pretty simple, right? Not only can we use an expression in the
<code>FROM</code> clause, but we can use <code>AS</code> to name it so that it
can be treated exactly like a table. Now all we have to do is figure out how
to construct such <code>SELECT</code> statements within our functions.</p>

    <h2>From Arrays to Sets</h2>

<p>Now, if we want to be able to take advantage of bulk updates in our
collection functions, we'll need some way to create a <code>SELECT</code>
statement that generates a row for every value in the array passed to the
function. Then we can use that <code>SELECT</code> statement as part of
the <code>INSERT</code> and <code>UPDATE</code> statements to update the
collections with the same number of queries regardless of how many IDs are in
the array.</p>

<p>When I first took on this problem, my first attempt was to create a
function that converts an array into a set. Fortunately, this turned out to be
pretty easy, given the way that set returning functions work:</p>

<pre>
CREATE OR REPLACE FUNCTION array_to_set(
    arr anyarray
) RETURNS SETOF anyelement AS $$
BEGIN
    FOR idx IN array_lower(arr, 1)..array_upper(arr, 1) LOOP
        RETURN NEXT arr[idx];
    END LOOP;
END;
$$ LANGUAGE plpgsql;
</pre>

<p>PostgreSQL provides a number of
<a href="http://www.postgresql.org/docs/current/static/datatype-pseudo.html"
title="PostgreSQL Pseudo-Types"><dfn>pseudo-types</dfn></a>, which are
convenient for contexts that need to handle any number of different data
types. In this example, we don't really care about the data type of the array
we're converting into a set, so we declare that <code>array_to_set()</code>
can take any kind of array—denoted as <code>anyarray</code>—as an argument,
and that it returns a <code>SETOF anyelement</code>. And indeed this function
works pretty well:</p>

<pre>
try=% <strong>select * from array_to_set( ARRAY[3,4,10,56,2] );</strong>
 array_to_set 
--------------
            3
            4
           10
           56
            2
(5 rows)
</pre>

<p>But then I realized that I needed not only each value in the array, but
also its position in the array, so that I could populate the ordering column
in the collection table. That would be trickier to write, given the limited
return values of a function, but fortunately, there is another approach.</p>

<h2>Series Generation</h2>

<p>PostgreSQL comes with a very nice set returning function called
<code>generate_series()</code>. This function takes three arguments: a
beginning number, an ending number, and an optional <q>step</q> number, and
returns a set of numbers from the beginning number to the ending number, each
incremented by the step number. It's easiest to understand when seen in
action:</p>

<pre>
try=% <strong>select * from generate_series(2, 4);</strong>
 generate_series 
-----------------
               2
               3
               4
(3 rows)

try=% <strong>select * from generate_series(1, 10, 2);</strong>
 generate_series 
-----------------
               1
               3
               5
               7
               9
(5 rows)
</pre>

<p>Nice, eh? Well, it's real power for solving the problem at hand is in
allowing us to create a <code>SELECT</code> statement that creates rows for
each number in the series and each value in an array, all without having to
call my <code>array_to_set()</code> function at all:</p>

<pre>
try=% <strong>SELECT gs.ser, coll.ids[gs.ser] as id</strong>
try-% <strong>FROM   (SELECT ARRAY[ 2,4,6,7,8 ]) AS coll(ids),</strong>
try-% <strong>       generate_series(1, 5) AS gs(ser);</strong>
 ser | id
-----+----
   1 |  2
   2 |  4
   3 |  6
   4 |  7
   5 |  8
(5 rows)
</pre>

<p>Because PostgreSQL <code>FROM</code> clauses may be SQL expressions, we can
simply call <code>generate_series()</code> in the <code>FROM</code> clause and
use its values to iterate over the array for each row. The key to being able
to reference the series is the <code>AS gs(ser)</code> clause, which gives the
call to <code>generate_series()</code> the table name <var>gs</var> and its
lone column the name <var>ser</var>. So then we can simply reference the
appropriate value in each row as <var>gs.ser</var>.</p>

<p>Thus, by using the <code>generate_series()</code> set returning function,
we're able to get at each element in the array for each row, while also
outputting the series number, all in a single query. This is <em>exactly</em>
what we need in order to use batch updates in the collection management
functions. So let's get on with it!</p>

<h2>Improved Tag Adding Function</h2>

<p>Just as a reminder, here's what the original
<code>entry_coll_tag_add()</code> function looked like:</p>

<table>
<tr><td><pre class="lines">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre></td><td>
<pre>
CREATE OR REPLACE FUNCTION entry_coll_tag_add (
    obj_id   integer,
    coll_ids integer[]
) RETURNS VOID AS $$
DECLARE
    last_ord smallint;
    got_ord  boolean;
    iord     integer  := 1;
BEGIN
    PERFORM true FROM entry WHERE id = obj_id FOR UPDATE;

    SELECT INTO last_ord COALESCE(max(tag_order), 0)
    FROM   entry_coll_tag
    WHERE  entry_id = obj_id;

    FOR iloop IN 1..array_upper(coll_ids, 1) LOOP
        IF coll_ids[iloop] IS NULL THEN
            CONTINUE;
        END IF;

        SELECT INTO got_ord true 
        FROM   entry_coll_tag
        WHERE  entry_id = obj_id
               AND tag_id = coll_ids[iloop];

        IF got_ord IS NULL THEN
            INSERT INTO entry_coll_tag (entry_id, tag_id, tag_order)
            VALUES (obj_id, coll_ids[iloop], last_ord + iord);
            iord := iord + 1;
        END IF;
    END LOOP;
END;
$$ LANGUAGE plpgsql;
</pre></td></tr></table>

<p>The bit that we want to eliminate is the loop. It's going to be a bit
tricky, because it does a <code>SELECT</code> to see if a record already
exists, and only does the insert if it doesn't already exist. That won't work
for the batch insert, but because it uses an expression to make it batched, we
can of course use a <code>WHERE</code> clause to limit the array values that
actually get inserted. So let's see what it looks like:</p>

<table>
<tr><td><pre class="lines">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td>
<pre>
CREATE OR REPLACE FUNCTION entry_coll_tag_add (
    obj_id   integer,
    coll_ids integer[]
) RETURNS VOID AS $$
DECLARE
    last_ord smallint;
BEGIN
    PERFORM true FROM entry WHERE id = obj_id FOR UPDATE;

    SELECT INTO last_ord COALESCE(MAX(ord), 0)
    FROM   entry_coll_tag
    WHERE  entry_id = obj_id;

    INSERT INTO entry_coll_tag (entry_id, tag_id, ord )
    SELECT obj_id, coll_ids[gs.ser], gs.ser + last_ord
    FROM   generate_series(1, array_upper(coll_ids, 1)) AS gs(ser)
    WHERE  coll_ids[gs.ser] NOT IN (
        SELECT tag_id FROM entry_coll_tag
        WHERE entry_id = obj_id
    );

END;
$$ LANGUAGE plpgsql;
</pre></td></tr></table>

<p>The first difference to note is the reduced number of declared variables.
Since we're using <code>generate_series()</code> to generate the order numbers
and not using a <code>SELECT</code> statement, we no longer need the
<var>got_ord</var> or <var>iord</var> variables (although more on
<var>iord</var> in a bit).</p>

<p>Then, the first two statements in the body of the function are the same as
before, namely the <code>PERFORM</code> statement that locks the entry record,
and the <code>SELECT</code> statement that determines the highest existing
value for the <var>tag_order</var> column for the entry ID stored in
<var>obj_id</var>. But then, rather than the loop, we have a single
<code>INSERT</code> statement. Let's take a closer look at what it's
doing.</p>

<ul>
  <li>Line 14 starts the <code>INSERT</code> statement as usual.</li>
  <li>Line 15 starts the <code>SELECT</code> statement that makes up the
    expression part of the <code>INSERT</code> statement. It uses the
    <var>obj_id</var> variable for the <var>entry_id</var> column,
    <code>coll_ids[gs.ser]</code> for the <var>tag_id</var> column,
    and <code>gs.ser + last_ord</code> for the <var>tag_order</var> column.
    Where did <var>gen.ser</var> come from? I'm so glad you asked!</li>
  <li>Line 16 uses <code>generate_series()</code> to create a series that
    corresponds to the number of items in the <var>coll_ids</var> array,
    using <code>array_upper()</code> to determine that number. Because
    PostgreSQL allows an expression to be used in a <code>FROM</code>
    clause, we can just treat the values returned by
    <code>generate_series()</code> as a table with a single column. The
    <code>AS gen(ser)</code> syntax gives us a convenient name for the
    <q>table</q> (<code>gen</code>) and its lone column
    (<code>ser</code>).</li>
  <li>Lines 17-20 contain the <code>WHERE</code> clause for the
    <code>SELECT</code> statement. It uses a sub-query to ensure that we don't
    insert any tag IDs in the <var>coll_ids</var> array that are already
    associated with the entry ID in <var>obj_id</var>. Unlike the original
    version of this function, we don't have to check for <code>NULL</code>
    values because they are handled by this <code>WHERE</code> expression:
    <code>NULL</code> will never be <code>IN</code> any list of values.</li>
</ul>

<p>And that's it. Really! No matter how many tag ID are passed in the
<var>coll_ids</var> array, it will execute no more than three queries,
total. Given that the previous version of the function would execute
<em>four</em> queries when <var>coll_ids</var> had but a single ID, and that
two more queries could be added for every additional ID, it's clear that this
version, with its constant number of queries, is a big win. And the fact that
it actually consists of less code doesn't hurt, either.</p>

<p>That said, this function does not behave identically to the original, in that
it may not insert the new IDs with perfectly sequential values for the
<var>tag_order</var> column. This was the purpose served by <var>iord</var> in
the previous version of the function, but there's place for it in the new
version. For example, say the function was called thusly:</p>

<pre>SELECT entry_coll_tag_add(1, '{5,13,65,12}');</pre>

<p>And say that entry ID 1 already had tag IDs 13 and 65 associated with it.
In such a case, the result might look something like this:</p>

<pre>
try=% <strong>SELECT * FROM entry_coll_tag WHERE entry_id = 1 ORDER BY tag_order;</strong>
 entry_id | tag_id | tag_order
----------+--------+-----------
        1 |     13 |         1
        1 |     65 |         2
        1 |      5 |         3
        1 |     12 |         6
(4 rows)
</pre>

<p>Note how the <var>tag_order</var> jumps from 3 to 6. This is because IDs 13
and 65 would have been ordered 4 and 5, but since they weren't inserted,
neither were their order numbers. In truth, this situation existed already,
thanks to the <code>entry_coll_tag_del()</code> function created in the
previous article, since that function deletes certain tag IDs without
resetting the <var>tag_order</var> of any remaining rows. But in most
applications this shouldn't matter, as long as you use <var>tag_order</var>
purely for ordering, rather than for ordinal positional values. It does not
represent array indexes, just ordering.</p>

<h2>Improved Tag Setting Function</h2>

<p>And what of <code>entry_coll_tag_set()</code>? Let's look at the
original.</p>

<table>
<tr><td><pre class="lines">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td>
<pre>
CREATE OR REPLACE FUNCTION entry_coll_tag_set (
    obj_id   integer,
    coll_ids integer[]
) RETURNS VOID AS $$
BEGIN
    PERFORM true FROM entry WHERE id = obj_id FOR UPDATE;

    UPDATE entry_coll_tag
    SET    tag_order = -tag_order
    WHERE  entry_id = obj_id

    FOR iloop IN 1..array_upper(coll_ids, 1) LOOP
        IF coll_ids[iloop] IS NULL THEN
            CONTINUE;
        END IF;

        UPDATE entry_coll_tag
        SET    tag_order = iloop
        WHERE  entry_id = obj_id
               AND tag_id = coll_ids[iloop];

        IF FOUND IS false THEN
            INSERT INTO entry_coll_tag (entry_id, tag_id, tag_order)
            VALUES (obj_id, coll_ids[iloop], iloop);
        END IF;
    END LOOP;

    DELETE FROM entry_coll_tag
    WHERE  entry_id = obj_id AND tag_order &lt; 0;
END;
$$ LANGUAGE plpgsql;
</pre></td></tr></table>

<p>The number of queries run by this function is even more variable than the
number to be run by <code>entry_coll_tag_add()</code>, as each ID in
<var>coll_ids</var> will trigger the execution of either one or two queries:
one if the <code>UPDATE</code> statement updates a row, and two if it doesn't.
Now let's take a look at the batch updating version:</p>

<table>
<tr><td><pre class="lines">
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
</pre></td><td>
<pre>
CREATE OR REPLACE FUNCTION entry_coll_tag_set (
    obj_id   integer,
    coll_ids integer[]
) RETURNS VOID AS $$
BEGIN
    PERFORM true FROM entry WHERE id = obj_id FOR UPDATE;

    UPDATE entry_coll_tag
    SET    ord = -ord
    WHERE  entry_id = obj_id;

    IF FOUND IS false THEN
        INSERT INTO entry_coll_tag (entry_id, tag_id, ord)
        SELECT obj_id, coll_ids[gs.ser], gs.ser
        FROM   generate_series(1, array_upper(coll_ids, 1))
               AS gs(ser)
        WHERE  coll_ids[gs.ser] IS NOT NULL;
    ELSE
        UPDATE entry_coll_tag SET ord = ser
        FROM (
            SELECT gs.ser, coll_ids[gs.ser] as move_tag
            FROM   generate_series(1, array_upper(coll_ids, 1)) AS gs(ser)
            WHERE  coll_ids[gs.ser] IS NOT NULL 
        ) AS expansion
	    WHERE move_tag = entry_coll_tag.tag_id
              AND entry_id = obj_id;

        INSERT INTO entry_coll_tag (entry_id, tag_id, ord )
        SELECT obj_id, coll_ids[gs.ser], gs.ser
        FROM   generate_series(1, array_upper(coll_ids, 1)) AS gs(ser)
        WHERE  coll_ids[gs.ser] NOT IN (
            SELECT tag_id FROM entry_coll_tag
            WHERE  entry_id = obj_id
        );

        DELETE FROM entry_coll_tag
        WHERE  entry_id = obj_id AND ord &lt; 0;
    END IF;        
END;
$$ LANGUAGE plpgsql;
</pre></td></tr></table>

<p>Okay, I admit that, at first glance, this looks more complicated that the
original. But that's only because I added an optimization that I honestly
could have had in the original, had I noticed it before. So let's look at it
piece by piece.</p>

<ul>
  <li>Once again, the first two statements in the function body are the same
    as before. Here we retain the entry row locking <code>PERFORM</code>
    statement and the <code>UPDATE</code> statement that negates the values
    for the <var>tag_order</var> column for all records associated with
    <var>obj_id</var>.</li>
  <li>Line 12 is where the changes begin, and is also where I added the
    optimization missing in the original. If the globally-available PL/pgSQL
    boolean variable <code>FOUND</code> is <code>false</code>, it means that
    the <code>UPDATE</code> statement in lines 8-10 updated no records. That
    means that we don't have to do any fancy tricks to update
    previously-existing rows, since there are none. So…</li>
  <li>Lines 13-17 use <code>generate_series()</code> in a batch
    <code>INSERT</code> statement to simply insert <em>all</em> of the tag IDs
    in <var>coll_ids</var>. This is about as clear as it gets, with
    the <var>gs.ser</var> <q>column</q> both accessing individual array
    values as well as supplying the corresponding value for
    <var>tag_order</var>. We use the <code>WHERE coll_ids[gs.ser] IS NOT
    NULL</code> expression to skip over any <code>NULL</code> values that
    might be in <var>coll_ids</var>.</li>
  <li>Line 18 starts the block that handles the case where the
    <code>UPDATE</code> statement at lines 8-10 <em>did</em> update one or
    more rows. The upshot is that any tag IDs in <var>coll_ids</var> that
    are already in the table must be updated to their appropriate
    <var>tag_order</var> values, those that don't exist must be inserted,
    and any that are left over must be deleted.</li>
  <li>Accordingly, lines 19-26 use a batch <code>UPDATE</code> to update any
    existing records with new <var>tag_order</var> values. It uses
    <code>generate_series()</code> in the <code>FROM</code> clause to create
    the appropriate rows to be updated, with a <code>WHERE</code> clause in
    the expression that creates the <var>gs</var> table to skip
    <code>NULL</code> values and a <code>WHERE</code> clause for
    the <code>UPDATE</code> statement itself to map the <code>UPDATE</code> to
    the appropriate rows, where the entry ID and tag ID match.</li>
  <li>Rows 28-34 then insert the new tags into the collection table, once
    again using a batch <code>INSERT</code> statement with
    <code>generate_series()</code> to create the new rows to be added. As
    in our revamped <code>entry_coll_tag_add()</code> function, the
    <code>WHERE</code> clause skips <code>NULL</code>s and prevents existing
    tags from being inserted and conflicting with the unique index.</li>
  <li>And finally, rows 36-37 delete any leftover associations, just as it did
    before, by deleting those with a <var>tag_order</var> less than zero.</li>
</ul>

<p>Phew! That was a lot to cover, but the result is that this function now
also runs a constant number of queries for each execution. Or nearly constant.
If there are no existing rows in the collection table, then it will run only
three queries. Otherwise it will run five. But in either case, it will never
run any more than five, ever.</p>

<h2>Benchmarks</h2>

<p>So how does this refactoring translate into performance? It's hard to say
really, since the performance of the previous iterations of the functions
depends on how many IDs were in <var>coll_ids</var>. Nevertheless, I reworked
my original benchmarking program into a new version that compares the new
functions to the old, plus the original Perl version, just as a control. The
results:</p>

<pre>
batch:  8.27 wallclock seconds (0.02 usr + 0.56 sys = 0.58 CPU) @ 36.28/s
 func: 10.54 wallclock seconds (0.03 usr + 0.56 sys = 0.59 CPU) @ 28.47/s
 perl: 16.46 wallclock seconds (0.11 usr + 2.77 sys = 2.88 CPU) @ 18.23/s
</pre>

<p>So, assuming that my benchmarking script tests a <q>typical</q> number of
objects in the collection, the constant functions (with the label
<q>batch</q>) are about 27.5% faster than the looping versions (<q>func</q>),
while running nearly twice as fast as doing all the work from within Perl
(which also loops). Yay for batch updates!</p>

<p><a
href="http://www.justatheory.com/code/bench_plpgsql_collection_batch_functions.tgz"
title="Download the benchmark test">Download the test</a> to test the
difference for yourself.</p>

<h2>The Race Condition Again</h2>

<p>In the <a href="plpgsql_collection_functions.html" title="Practical
PL/pgSQL: Managing Ordered Sets">previous article</a> in this series, I wrote
that the race condition between two processes updating the same collection has
been eliminated. Well, that was a little misleading. It was <em>almost</em>
eliminated. One of the reviewers pointed out that there now exists a
different—and much more subtle—race condition.</p>

<p>The issue is that, although we lock the appropriate <var>entry</var> table
row to prevent inserts from being executed until we're done updating the
collection, foreign key constraints are checked <em>after</em> an
<code>INSERT</code> has been executed. This is the reverse of how our
functions do it: They lock the <var>entry</var> row <em>before</em> they
do anything else.</p>

<p>So why is this a problem? Consider two connections to the database, one
updating a collection via our functions and the other updating the same
collection via manual queries.</p>

<ol>
  <li>Manual connection inserts into <var>entry_coll_tag</var></li>
  <li>Function connection gets lock on <var>entry</var> row</li>
  <li>Manual connection attempts to check the foreign key constraint, thus
      requesting a lock on <var>entry</var>. It blocks while it waits for
      the function connection to commit</li>
  <li>Function connection makes a conflicting insert into the collection</li>
  <li>Function connection waits on the Manual connection as part of checking
      the foreign key
      constraint. Result: <a href="http://en.wikipedia.org/wiki/Deadlock"
      title="Wikipedia: Deadlock">deadlock</a>.</li>
</ol>

<p>The key here is step 2: If the function gets a lock after the manual
connection has inserted the new row but <em>before</em> it checks the foreign
key constraint, and makes a conflicting insert, there <em>will</em> be a
deadlock: the connections wait for each other to finish, and therefore neither
of them ever does. This is a very narrow race condition—much narrower than the
original race condition—but it is still very real.</p>

<p>So what's the solution to this problem? It's simple, really: never allow
any connection to use anything other than your functions to update a
collection. The functions always acquire the lock before doing anything else,
so there's no way that one connection using the functions can get into a
deadlock with another connection using the functions. And the best way to
enforce this rule is with PostgreSQL permissions. For example, say that you
let applications connect to your database using a PostgreSQL user
named <q>appuser</q>. First, revoke that user's permission to make changes to
the <var>entry_coll_tag</var> table:</p>

<pre>
try=# <strong>REVOKE INSERT, UPDATE, DELETE ON entry_coll_tag</strong>
try-# <strong>FROM   appuser;</strong>
REVOKE
</pre>

<p>Then, create the functions as a user that <em>has</em> these permissions,
and use the <code>SECURITY DEFINER</code> at the end of each function
declaration, like so:</p>

<pre>
-- ...
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
</pre>

<p>The <code>SECURITY DEFINER</code> clause allows the function to be executed
with the permissions of the user that <em>defined</em> the function, rather
than the user <em>calling</em> it. Since it was created by a privileged user,
unprivileged users will be able to use the function to make changes to the
collection table even if they can't change the collection table directly.</p>

<h2>Conclusion</h2>

<p>Functions that perform a finite number of tasks are nearly always
preferable to those that perform a variable number of tasks based on the
number of items to process. PostgreSQL kindly provides the tools to eliminate
looping constructs in linear functions. By using the
<code>generate_series()</code> function and PostgreSQL's batch update syntax,
it's relatively straight-forward to eliminate array-based looping constructs
in PL/pgSQL functions. And now that those tools are at your disposal, get out
there and take advantage of them!</p>

<h2>Acknowledgements</h2>

<p>My thanks to Josh Berkus for showing me how to use
<code>generate_series()</code> and batch updates to eliminate the loops in my
original collection functions. I learned a great deal in the process, and am
pleased to pass on the benefits of Josh's knowledge here. I'm also grateful to
<code>AndrewSN</code> for pointing out the race condition and explaining it to
me.</p>

  </body>
</html>

